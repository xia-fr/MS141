{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS 141 Lecture 10\n",
    "# Root Finding, Optimization and Non-linear Systems\n",
    "\n",
    "## Read: Chapter 6 of Newman's book (pages 263-285)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['font.size'] = 16\n",
    "rcParams['figure.figsize'] = (12,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\newcommand{\\bx}{{\\mathbf{x}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Root finding\n",
    "Root finding refers to the general problem of looking for a solution to an equation  $f(x)=0$, namely finding the zeroes (or roots) of a function $f(x)$.<br> \n",
    "It is a very general problem with broad applications in scientific computing. For example, if we want to optimize a function  $f(x)$, we need to find its minimum or maximum and therefore solve the equation  $f'(x)=0$. Therefore, *optimization* $-$ finding maxima and minima $-$ corresponds to finding the zeros of the first derivative (or gradient in multiple dimensions). We will first focus on functions of one variable. \n",
    "\n",
    "While in some cases an exact solutions to $f(x)=0$ exists, typically one has to find approximate solutions numerically. \n",
    "The quadratic formula\n",
    "\n",
    "$$ x_{\\pm} = \\frac{-b \\pm \\sqrt{b^2 - 4 a c}}{2a}$$\n",
    "\n",
    "gives an exact method for finding the roots of the equation\n",
    "$$f(x) = a x^2 + b x + c = 0$$ \n",
    "\n",
    "There is a general formula to solve a cubic equation and even a quartic (degree 4) equation, but there is no formula for the zeros of a degree 5 or higher polynomial (Abel-Ruffini theorem). There are many more examples of equations with no known analytic solution. \n",
    "\n",
    "Our strategy will be to use numerical methods to find approximate solutions. We will examine three - the Bisection, Secant, and Newton methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bisection method\n",
    "\n",
    "The simplest root finding algorithm is the __[bisection method](https://en.wikipedia.org/wiki/Bisection_method)__. The algorithm applies to any continuous function $f(x)$ on an interval $[a,b]$ where the value of the function $f(x)$ changes sign from $a$ to $b$. The idea is simple: divide the interval into two parts; since a solution must exist within one of the two subintervals, select the subinterval where the sign of $f(x)$ changes, and then repeat.<br>\n",
    "\n",
    "The main advantage of the bisection method is its simplicity. The main disadvantages are that it converges slowly because it does not use information about the function (such as its derivative), and it cannot be generalized to more than one dimension.\n",
    "\n",
    "The bisection algorithm finds roots as follows:\n",
    "1. Choose a starting interval $[a_0,b_0]$ such that $f(a_0)f(b_0)<0$ ( $f$ changes sign in the interval).\n",
    "2. Compute $f(m_0)$ where $m_0 = \\frac{a_0 + b_0}{2}$ is the midpoint.\n",
    "3. Determine the next subinterval $[a_1,b_1]$:<br>\n",
    "    a. If $f(a_0)f(m_0)<0$, then set the next interval between $a_1 = a_0$ and $b_1 = m_0$.<br>\n",
    "    b. If $f(b_0)f(m_0)<0$, then set the next interval between $a_1 = m_0$ and $b_1 = b_0$.<br>\n",
    "4. Repeat (2) and (3) for a given number of cycles $N$, after which the interval reaches a length of $\\frac{b-a}{2^N}$. \n",
    "5. Return the midpoint value as the solution. Its error relative to the exact solution will be of order $\\frac{b-a}{2^{N+1}}$.<br>\n",
    "If one selects a small threshold $\\varepsilon$ for the solution, such that $\\varepsilon = \\frac{b-a}{2^{N}}$, this threshold is reached in roughly $N = \\log(\\frac{b-a}{\\varepsilon})$ steps.\n",
    "(Image source: Wiki)\n",
    "<img src=\"images/Bisection_method.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Root finding using the bisection method \n",
    "'''\n",
    "a,b: interval limits\n",
    "f: function whose roots we are searching \n",
    "N: number of iterations \n",
    "Returns: midpoint of the Nth interval (approximate root of f)\n",
    "'''\n",
    "\n",
    "def bisection(f,a,b,N): #alternatively, could pass a threshold\n",
    "    if f(a)*f(b) >= 0:\n",
    "        print(\"Bisection method fails.\")\n",
    "        return None\n",
    "    a_n = a\n",
    "    b_n = b\n",
    "    for n in range(1,N+1):\n",
    "        # midpoint\n",
    "        m_n = (a_n + b_n)/2\n",
    "        \n",
    "        # zero is left of midpoint\n",
    "        if f(a_n)*f(m_n) < 0:\n",
    "            a_n = a_n\n",
    "            b_n = m_n\n",
    "            \n",
    "        # zero is right of midpoint    \n",
    "        elif f(b_n)*f(m_n) < 0:\n",
    "            a_n = m_n\n",
    "            b_n = b_n\n",
    "            \n",
    "        elif f(m_n) == 0:\n",
    "            print(\"Found exact solution.\")\n",
    "            return m_n\n",
    "        \n",
    "        else:\n",
    "            print(\"Bisection method fails.\")\n",
    "            return None\n",
    "        \n",
    "    return (a_n + b_n)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found exact solution.\n",
      "1.618033988749895\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Solve x^2 - x - 1 = 0\n",
    "f = lambda x: x**2 - x - 1\n",
    "approx_phi = bisection(f,1,2,50) #50\n",
    "print(approx_phi)\n",
    "\n",
    "# Check solution\n",
    "print (f(approx_phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found exact solution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solve (2x-1)(x-3) = 0\n",
    "f = lambda x: (2*x - 1)*(x - 3)\n",
    "bisection(f,0,1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bisection method with SciPy\n",
    "\n",
    "SciPy's `optimize.bisect` implements the bisection method for a user-defined number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6180339887487207"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "f = lambda x: x**2 - x - 1\n",
    "optimize.bisect(f, 1, 2, maxiter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda x: (2*x - 1)*(x - 3)\n",
    "optimize.bisect(f, 0, 1, maxiter=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Secant method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __[secant method](https://en.wikipedia.org/wiki/Secant_method)__ is a root-finding algorithm that uses a succession of secant lines to better approximate the root of a function.<br> \n",
    "The secant method can be thought of as a finite-difference approximation of Newton's method (see below).<br> \n",
    "\n",
    "However, the method was developed independently of Newton's method and predates it by over 3,000 years.<br> \n",
    "It relies on the simple assumption that near a root a function is usually linear.\n",
    "\n",
    "Consider the line connecting the endpoint values $(x_0,f(x_0))$ and $(x_1,f(x_1))$ of a function $f(x)$.<br> \n",
    "The line connecting these two points is called the secant line and is given by the formula \n",
    "\n",
    "$$ y(x) = \\frac{f(x_1) - f(x_0)}{x_1-x_0} (x-x_1) + f(x_1)$$\n",
    "\n",
    "The secant crosses the $x$ axis when $y=0$, and thus at the point $x=x_2$:\n",
    "\n",
    "$$ x_2 = x_1 - f(x_1) \\frac{x_1-x_0}{f(x_1)-f(x_0)}.$$\n",
    "\n",
    "The next secant is built between the points $(x_1, f(x_1))$ and $(x_2, f(x_2))$, and we call $x_3$ its intersection with the $x$ axis. We continue this process, solving for $x_3$, $x_4$, etc., until we reach a sufficient precision $-$ that is, a sufficiently small difference between $x_n$ and $x_{nâˆ’1}$.\n",
    "\n",
    "The secant algorithm can be summarized as follows:<br>\n",
    "1. Choose a starting interval $[x_0,x_1]$ such that $f(x_0)f(x_1)<0$ ( $f$ changes sign in the interval).\n",
    "2. Find $x_2 = x_{1}-f(x_{1}){\\frac {x_{1}-x_{0}}{f(x_{1})-f(x_{0})}}$.\n",
    "3. Repeat the procedure in the next subinterval $[x_1,x_2]$, solving for $x_3$, and so on.<br> \n",
    "   At step $n$, the next iterate solution is\n",
    "   \n",
    "$$ \\boxed{x_{n+1}=x_{n}-f(x_{n})\\,{\\frac {x_{n}-x_{n-1}}{f(x_{n})-f(x_{n-1})}}}$$\n",
    "\n",
    "4. Repeat steps 2) and 3) until the interval $[x_N,x_{N+1}]$ reaches a predetermined length. \n",
    "5. Return the value $x_N$, the $x$-intercept of the $N^{\\rm{th}}$ subinterval. This will be the approximate solution to $f(x)=0$. <br>\n",
    "(Image source: Wiki)\n",
    "<img src=\"images/Secant_method.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that unlike the bisection method, the secant method does not require that the root remain bracketed.<br> \n",
    "If the initial values are not close enough to the root, there is no guarantee that the secant method will converge.\n",
    "When it converges, the secant method converges faster than bisection as it uses knowledge of the function to find its roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secant(f,x0,x1,N):\n",
    "    x_old = x0 \n",
    "    x_new = x1 \n",
    "    \n",
    "    eps=1.e-10\n",
    "    for i in range(N):\n",
    "        if (abs(f(x_new)-f(x_old)) < eps):\n",
    "            return x_new\n",
    "        \n",
    "        # find x_n+1\n",
    "        x_tmp = x_new - 1.0*f(x_new)*(x_new-x_old)/(f(x_new)-f(x_old))\n",
    "        \n",
    "        # debug: print root\n",
    "        # print (x_tmp)\n",
    "        \n",
    "        x_old = x_new\n",
    "        x_new = x_tmp\n",
    "        \n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6180339887498947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.220446049250313e-16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solve x^2 - x - 1 = 0\n",
    "f = lambda x: x**2 - x - 1\n",
    "root = secant(f,1,2,25)\n",
    "print(root)\n",
    "\n",
    "# Check solution\n",
    "f(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secant method with SciPy\n",
    "\n",
    "The `optimize.newton` function uses the secant method if a derivative of f is not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6180339887498947\n"
     ]
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "f = lambda x: x**2 - x - 1\n",
    "# use x_0=1.3\n",
    "root = optimize.newton(f, 1.3)\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Newton method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Newton method](https://en.wikipedia.org/wiki/Newton%27s_method) (also known as Newton-Raphson method) is a widely used root finding approach. It uses a single point as the initial guess of the root of a function, and using the direction of the tangent it finds successively better approximations to the root. It requires the *computation of the derivative*.<br> \n",
    "It can be implemented in one or multiple dimensions. We will first investigate its implementation in one dimension.<br>\n",
    "\n",
    "The idea of the Newton method is that if an initial guess point $x_0$ is near a solution of $f(x)=0$, then we can approximate $f(x)$ by the tangent line at $x_0$ <br> \n",
    "and compute the $x$-intercept of the tangent line. The equation of the tangent line at $x_0$ is<br>\n",
    "\n",
    "$$ y (x) = f'(x_0) (x-x_0) + f(x_0)$$\n",
    "\n",
    "The $x$-intercept is the solution $x_1$ of the equation $y=0$, which gives<br>\n",
    "\n",
    "$$ x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$$\n",
    "\n",
    "Repeating this procedure, we obtain a zero of $f$ from sequence of values $x_n$ given by the recursive formula\n",
    "\n",
    "$$\\boxed {x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)},}$$\n",
    "\n",
    "through which one progressively finds better approximations of the solution to $f(x)=0$.\n",
    "When it converges, Newton's method usually converges quickly and this is its main advantage. However, Newton's method is not guaranteed to converge and this is obviously a disadvantage, especially compared to the bisection method, which is guaranteed to converge to a solution (provided it starts with an interval containing a root). \n",
    "\n",
    "Newton's method requires computing values of the derivative of the function. This is potentially a disadvantage if the derivative is difficult to compute (e.g., in multiple dimensions). [Newton's method for optimization](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is applied to the derivative $f'(x)$ of a twice-differentiable function $f(x)$ to find the roots of the derivative (the solutions to $f'(x) = 0)$. For optimization, the maxima / minima of $f(x)$ are obtained from the sequence: \n",
    "\n",
    "$$\\boxed{ x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)}, }$$\n",
    "\n",
    "which involves computing the second derivatives of $f$.\n",
    "\n",
    "(Image source: Wiki)\n",
    "<img src=\"images/Newton_method.gif\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of Newton's method. We need to provide the derivative $f'(x)$ as input or as a callable function. In our case, we will compute the derivative analytically, but in general a numerical approximation of $f'(x)$ is needed. The algorithm stops either when the function is less than a small threshold value and thus close enough to zero,  $f(x_n) < \\varepsilon$, or after a maximum number of iterations. Also, if $f'(x_n)=0$, the algorithm fails and needs to be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f,Df,x0,epsilon,max_iter):\n",
    "    '''Approximate solution of f(x)=0 by Newton's method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : function in f(x)=0.\n",
    "    Df : derivative function of f(x).\n",
    "    x0 : number\n",
    "        Initial guess for a solution f(x)=0.\n",
    "    epsilon : number\n",
    "        Stopping criterion abs(f(x)) < epsilon.\n",
    "    max_iter : integer\n",
    "        Maximum number of iterations of Newton's method.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xn : number\n",
    "    '''\n",
    "    xn = x0\n",
    "    for n in range(0,max_iter):\n",
    "        \n",
    "        fxn = f(xn)\n",
    "        \n",
    "        if abs(fxn) < epsilon:\n",
    "            print('Found solution after',n,'iterations.')\n",
    "            return xn\n",
    "        \n",
    "        if Df(xn) == 0:\n",
    "            print('Zero derivative. No solution found.')\n",
    "            return None\n",
    "        \n",
    "        xn = xn - fxn/Df(xn)\n",
    "        \n",
    "    print('No solution found in ', max_iter, ' iterations.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found solution after 5 iterations.\n",
      "1.618033988749989\n",
      "2.1049828546892968e-13\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: x**2 - x - 1\n",
    "Df = lambda x: 2*x - 1\n",
    "root = newton(f,Df,1,1e-12,10)\n",
    "print(root)\n",
    "#check\n",
    "print (f(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found solution after 5 iterations.\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: (2*x - 1)*(x - 3)\n",
    "Df = lambda x: 2*(x-3) + (2*x - 1)\n",
    "root = newton(f,Df,0.99,1e-10,10)\n",
    "print (root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton method with SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.618033988749895\n"
     ]
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "f = lambda x: x**2 - x - 1\n",
    "root = optimize.newton(f, 1.5, fprime=lambda x: 2*x - 1)\n",
    "print (root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: (2*x - 1)*(x - 3)\n",
    "root = optimize.newton(f, 1.5, fprime=lambda x: 2*(x-3) + (2*x - 1))\n",
    "print (root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Newton method in more than one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generalize the Newton method to more than one variable, it is useful to derive it using Taylor expansion:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "        y &= f(x_n) + (x - x_n) f'(x_n) \\\\\n",
    "        y & = 0 \\quad \\implies \\quad x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}.\n",
    "      \\end{aligned}$$\n",
    "\n",
    "For a function of more than one variable, the points in $N$-dimensional space become $\\bx = (x_1,x_2,\\ldots,x_N)$ and the derivative $f'(x)$ becomes the gradient $\\mathbf{\\nabla f} = \\frac{\\partial f}{\\partial \\bx}$. To solve $f(\\bx)=0$ we rewrite the Taylor expansion as:\n",
    "\n",
    "$$ \\boxed{ \\mathbf{\\nabla f} \\cdot (\\bx_{n+1} - \\bx_n) = -\\, f(\\bx_n). }$$\n",
    "\n",
    "Using this scheme, at each iteration step we can update the point $\\bx_n$ by solving one equation in $N$ unknowns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization.** It is far more common to use the multi-variable Newton approach to find local maxima or minima of $f$, which requires the gradient to vanish, $\\mathbf{\\nabla f} = 0$. Newton's method in one variable uses the first and second derivatives to find maxima or minima. In higher dimensions, Newton's method uses the gradient and the Hessian matrix (matrix of second derivatives) of the function to be minimized. Now the iteration involves the symmetric Hessian matrix:\n",
    "\n",
    "$$H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}.$$ \n",
    "\n",
    "At each step, Newton's scheme becomes a systems of $N$ linear equations in $N$ unknowns:\n",
    "\n",
    "$$ H (\\bx_{n+1} - \\bx_n) = -\\, \\mathbf{\\nabla f}(\\bx_n). $$\n",
    "\n",
    "Therefore, each step in Newton's algorithm requires updating the Hessian matrix plus $\\mathcal{O}(N^3)$ operations to solve a linear system.\n",
    "\n",
    "Computing the Hessian matrix at each step and solving a linear system is computationally costly. [*Quasi-Newton methods*](https://en.wikipedia.org/wiki/Quasi-Newton_method), such as the Broyden and BFGS methods, are widely used optimization schemes in which the Hessian matrix is not calculated explicitly, but rather only estimated approximately, for example using finite differences as in the secant approach. The iterative scheme becomes:\n",
    "\n",
    "$$\\bx_{n+1} - \\bx_n = -\\alpha_n B_n^{-1}\\, \\mathbf{\\nabla f}(\\bx_n),$$\n",
    "\n",
    "where $\\alpha_n$ is a line search parameter and $B_n$ an approximate Hessian. The gradient descent method we saw earlier can also be regarded as a quasi-Newton scheme with $B_n = I$ and appropriate $\\alpha_n$. \n",
    "Quasi-Newton methods are typically more robust than the pure Newton method itself, require no second derivative evaluation, and typically cost only $\\mathcal{O}(N^2)$ per iteration step. For this reason they are widely used in practice, particularly the [BFGS method](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm), which is available in SciPy as the `scipy.optimize.fmin_bfgs` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nonlinear systems.** Another generalization of the Newton method can be used to solve system of nonlinear equations.<br> The nonlinear system with $N$ equations and $N$ unknowns can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_1\\,(x_1,x_2, \\ldots, x_N) &= 0 \\\\\n",
    "f_2\\,(x_1,x_2, \\ldots, x_N) &= 0 \\\\\n",
    "\\vdots \\\\\n",
    "f_N\\,(x_1,x_2, \\ldots, x_N) &= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "or in more compact form, $\\mathbf{f}(\\bx)=0$. In this case, the gradient is generalized to the Jacobian:\n",
    "\n",
    "$$ J_{ij}(\\bx) = \\frac{\\partial f_i}{\\partial x_j} $$\n",
    "\n",
    "Newton's method becomes the matrix equation\n",
    "\n",
    "$$J (\\bx_n) \\cdot ( \\bx_{n+1} - \\bx_n ) = -\\, {\\boldsymbol{f}}(\\bx_n)$$\n",
    "\n",
    "which requires to solve a linear system at each step. In the algorithm, one starts with an initial guess $\\bx_0$ and computes $\\mathbf{f}(\\bx_0)$. At step $n$:\n",
    "\n",
    "1.  Compute ${\\boldsymbol{f}}(\\boldsymbol{x}_n)$.\n",
    "\n",
    "2.  Compute the Jacobian $J (\\boldsymbol{x}_n)$.\n",
    "\n",
    "3.  Solve for the search direction ${\\boldsymbol{s}} = \\bx_{n+1} - \\bx_n$ from\n",
    "    $J (\\boldsymbol{x}_n) {\\boldsymbol{s}} = -{\\boldsymbol{f}}(\\boldsymbol{x}_n)$.\n",
    "\n",
    "4.  Compute $\\boldsymbol{x}_{n+1} = \\boldsymbol{x}_n + {\\boldsymbol{s}}$.\n",
    "\n",
    "Note that the Jacobian is never inverted explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Nonlinear system example\n",
    "\n",
    "We illustrate Newton's method by solving the nonlinear system\n",
    "\n",
    "$$\\begin{aligned}\n",
    "        x_1 + 2\\,x_2 - 2 & = 0 \\\\\n",
    "        x_1^2 + 4\\,x_2^2 - 4 & = 0.\n",
    "      \\end{aligned}$$\n",
    "\n",
    "with Jacobian matrix\n",
    "\n",
    "$$ J =  \\begin{pmatrix}\n",
    "      1 & 2 \\\\\n",
    "      2\\,x_1 & 8\\,x_2\n",
    "    \\end{pmatrix}\n",
    "$$    \n",
    "\n",
    "We take $\\bx_0 = (1\\,\\,\\,2)^T$ as the initial guess, and attempt to find the exact solution $\\bx^* = (0\\,\\,\\,1)^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def newton_system(f, df, x0, nsteps=10):\n",
    "    \n",
    "    N = len(x0)\n",
    "    \n",
    "    x = np.zeros((N,nsteps+1))\n",
    "    x[:, 0] = x0\n",
    "    \n",
    "    for n in range(nsteps):\n",
    "        fx = f(x[:,n])\n",
    "        J = df(x[:,n])\n",
    "        \n",
    "        # J(x_n+1 - x_n) = - f(x_n)\n",
    "        dx = np.linalg.solve(J, -fx) \n",
    "        \n",
    "        x[:, n+1] = x[:,n] + dx\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterative results from Newton:\n",
      "\n",
      "[1. 2.]\n",
      "[-0.8333333333  1.4166666667]\n",
      "[-0.1893939394  1.0946969697]\n",
      "[-0.0150791353  1.0075395677]\n",
      "[-0.0001120013  1.0000560006]\n",
      "[-0.0000000063  1.0000000031]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    f = np.zeros_like(x)\n",
    "    f[0] = x[0] + 2*x[1] - 2.0\n",
    "    f[1] = x[0]**2 + 4.0*x[1]**2 - 4.0\n",
    "    return f\n",
    "\n",
    "def df(x):\n",
    "    df = np.zeros((len(x),len(x)))\n",
    "    df[0,0] = 1.0\n",
    "    df[0,1] = 2.0\n",
    "    df[1,0] = 2.*x[0]\n",
    "    df[1,1] = 8.*x[1]\n",
    "    return df\n",
    "\n",
    "nsteps = 10\n",
    "x0 = np.array((1.0,2.0))\n",
    "result = newton_system(f, df, x0, nsteps)\n",
    "\n",
    "print ('Iterative results from Newton:\\n')\n",
    "\n",
    "for i in range (nsteps+1):\n",
    "    np.set_printoptions(precision=10,suppress=True) #format\n",
    "    print (result[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "R. Fletcher, Practical Methods of Optimization, 2nd Ed. (Wiley)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
