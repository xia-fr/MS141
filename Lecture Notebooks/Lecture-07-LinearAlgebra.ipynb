{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS 141 Lecture 7 \n",
    "\n",
    "# Linear systems\n",
    "\n",
    "### Read: Chapter 6 Newman. Optional but recommended: Chapter 2 of Heath's book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear algebra calculations are ubiquitous in scientific computing. They are found in nearly all science and physics problems and in subfields as diverse as<br> data fitting and machine learning, computer graphics and image processing, ordinary and partial differential equations, network theory, etc.\n",
    "\n",
    "An essential task is solving a [system of linear equations](https://en.wikipedia.org/wiki/System_of_linear_equations) with the form\n",
    "\n",
    "\\begin{equation}\n",
    "  A {\\bf x} = {\\bf b},\n",
    "\\end{equation}\n",
    "\n",
    "where $A$ is a square matrix with size $N \\times N$ and ${\\bf b}$ is a vector with $N$ components. In a typical scenario, $A$ and $\\mathbf{b}$ are known, and we wish to solve for the unknown solution vector ${\\bf x}$. This task is equivalent to solving $N$ simultaneous linear equations for the $N$ unknown components of $\\mathbf{x}$. \n",
    "\n",
    "Recall that not all linear systems can be solved. If the matrix $A$ is nonsingular, which is equivalent to requiring that $\\det(A) \\ne 0$, then the matrix can be inverted, and there is a unique solution. We will work under this assumption. \n",
    "\n",
    "A seemingly obvious approach could be to compute the inverse of $A$. However, inverting matrices (especially large ones) is very computationally expensive, with cost of order $N^3$, where $N$ is the size of the matrix. **Matrix inversion is never used in practice** to solve linear systems, and is almost never used in practice in *any* computational method. \n",
    "\n",
    "Another bad idea would be to apply Cramer's rule, in which each component of the solution is computed as a ratio of determinants. This approach would be astronomically expensive even for relatively small matrices. This and other textbook approaches are mostly useful only as theoretical tools.\n",
    "\n",
    "There are two main families of approaches to solve linear systems of equations:\n",
    "- **Direct** approaches that obtain an exact solution (within numerical accuracy) with a finite number of steps. \n",
    "- **Iterative** approaches in which an approximate solution is found through successive approximations that converge to the exact solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Direct methods \n",
    "Most direct methods use the same core approach: convert the linear system to an equivalent problem that is easy to solve. \n",
    "\n",
    "Let us consider which linear systems are easy to solve. Obviously if the matrix is diagonal finding the solution is trivial.<br> \n",
    "More generally, it is straightforward to solve a linear system if the matrix is *triangular*. As an example, consider\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{pmatrix} 1 & 2 & 3 \\\\ 0 & 4 & 5 \\\\ 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 9 \\\\ 2 \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "We can perform **back substitution** by solving from the bottom to the top. We first read off and solve the equation defined by the bottom row of the matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "  2\\, x_3 = 2 \\qquad \\implies \\qquad x_3 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "We then move up to solve the equation defined by the second row from the bottom, using the solution we found for $x_3$:\n",
    "\n",
    "\\begin{equation}\n",
    "  4 x_2 + 5 x_3 = 9 \\qquad \\implies \\qquad 4 x_2 = 4 \\qquad \\implies \\qquad x_2 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "Finally, we solve the equation defined by the first row of the matrix, using the solution we found for $x_2$ and $x_3$:\n",
    "\n",
    "\\begin{equation}\n",
    "  x_1 + 2 x_2 + 3 x_3 = 6 \\qquad \\implies \\qquad x_1 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "Back substitution will work for any *upper* triangular matrix, provided that no diagonal element is zero, as is guaranteed by our assumption that the matrix is non-singular. If the matrix is lower triangular we would start at the top and work our way down, using **forward substitution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Gaussian elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to convert our linear system to an equivalent system with triangular form. By equivalent, we mean that the new system has the same solution as the original. We will aim to construct an upper triangular matrix. \n",
    "\n",
    "Think of each matrix row as corresponding to a single linear equation, and each column corresponding to a single unknown variable.<br> \n",
    "The **three basic row operations** we can be perform without changing the solutions are:\n",
    "\n",
    "1. Multiply a row by a non-zero scalar (equivalent to rescaling an equation).\n",
    "2. Add to one row a scalar multiple of another.\n",
    "3. Swap two rows (equivalent to changing the order of the equations).\n",
    "\n",
    "The operations need to be performed consistently on both the entries of the matrix $A$ *and* the right-hand side vector ${\\bf b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination)** is the basic algorithm to reduce the matrix $A$ with arbitrary elements to an upper triangular matrix.<br> \n",
    "This result is achieved by performing a sequence of row operations. Once the system is in upper triangular form, it can be solved by back substitution.\n",
    "\n",
    "The algorithm considers each column in turn, annihilating the elements below the diagonal in each column to obtain an upper triangular matrix:\n",
    "\n",
    "1. Consider column $j$, where $j = 1 \\ldots N$. Its diagonal element $a_{jj}$ is called the *pivot*. We will assume for now that all pivots are non-zero.\n",
    "2. To annihilate all entries in rows $i = j+1 \\ldots N\\,$ below the diagonal $a_{jj}$, we multiply row $j$ by the multiplier $m_i = - a_{ij}/a_{jj}$,<br> and then add the result, respectively, to row $i = j+1 \\ldots N\\,$. \n",
    "\n",
    "Explicitly, this means replacing the elements $a_{ik}$ in row $i > j$ as\n",
    "\\begin{equation}\n",
    "  \\boxed{a_{ik} \\rightarrow a_{ik} - \\frac{a_{ij}}{a_{jj}} a_{jk}.}\n",
    "\\end{equation}\n",
    "\n",
    "We see that when $k = j$ the coefficients $a_{ij}$ below the pivot will be set to zero, as required.\n",
    "\n",
    "If we repeat this algorithm for all columns $j$, each time looping over rows below column $j$, we will obtain an upper triangular matrix.<br> \n",
    "Let us write this algorithm using a modified version of the code provided in Newman's book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian elimination with back substitution\n",
    "import numpy as np\n",
    "\n",
    "def GaussianEliminationBack(A,b):\n",
    "    # Store size of system, consistency check\n",
    "    N = len(b)\n",
    "    assert(np.all(A.shape == (N, N)))\n",
    "    \n",
    "    for j in range(N):\n",
    "        \n",
    "        #check that pivot is non-zero\n",
    "        assert(A[j,j] != 0)\n",
    "        pivot = A[j,j]\n",
    "        \n",
    "        for i in range(j+1,N):\n",
    "            mult = -A[i,j]/pivot\n",
    "            \n",
    "            A[i,:] += mult*A[j,:] #update row i>j, boxed formula\n",
    "            b[i] += mult*b[j]  #update RHS vector\n",
    "    \n",
    "    # debug:\n",
    "    #print (A)\n",
    "    #print (b)\n",
    "    \n",
    "    # back substitution\n",
    "    x = np.zeros(N)\n",
    "    for i in range(N-1,-1,-1): #from the last row up\n",
    "        x[i] = b[i]/A[i,i]\n",
    "        \n",
    "        for j in range(i+1,N):\n",
    "            x[i] -= A[i,j]*x[j]/A[i,i]\n",
    "    \n",
    "    print('Solution: ', x)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution:  [ 2. -1. -2.  1.]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2., 1, 4, 1], \\\n",
    "              [3, 4, -1, -1], \\\n",
    "              [1, -4, 1, 5], \\\n",
    "             [2, -2, 1, 3]])\n",
    "\n",
    "b = np.array([-4., 3, 9, 7])\n",
    "\n",
    "# debug:\n",
    "# print (A)\n",
    "# print (b)\n",
    "\n",
    "x = GaussianEliminationBack(A,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciPy's result:\n",
      "[ 2. -1. -2.  1.]\n"
     ]
    }
   ],
   "source": [
    "# check the result with SciPy\n",
    "import scipy.linalg as la\n",
    "x = la.solve(A, b)\n",
    "\n",
    "print ('SciPy\\'s result:')\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Gaussian elimination with partial pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two severe limitations of plain Gaussian elimination:<br> \n",
    "1) The algorithm fails if the matrix $A$ acquires a zero pivot during the elimination process (or if $a_{11}=0$).<br> \n",
    "2) If too many pivots are smaller than 1, dividing by the pivots during Gaussian elimination will lead to progressively larger matrix elements, and thus lower accuracy due to build-up of rounding errors. \n",
    "This was one of the main concerns of Von Neumann and colleagues in the early day of scientific computing.\n",
    "\n",
    "The solution to these problems is as simple as swapping rows and choosing at each step the largest pivot available in a given column below the diagonal.<br> \n",
    "The approach of swapping rows to choose the largest pivot is called *partial pivoting* and is always used in practical implementations of Gaussian elimination.<br>\n",
    "\n",
    "The algorithm below implements Gaussian elimination with partial pivoting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian elimination with partial pivoting and backsubstitution (3 loops)\n",
    "import numpy as np\n",
    "\n",
    "def GaussianEliminationPivotingBack(A,b):\n",
    "    # Store size of system, consistency check\n",
    "    N = len(b)\n",
    "    assert(np.all(A.shape == (N, N)))\n",
    "    \n",
    "    for j in range(N): # Loop 1\n",
    "        \n",
    "        #find row with pivot of largest magnitude\n",
    "        max_row = np.argmax(abs(A[j:,j]))\n",
    "        #print (max_row)\n",
    "        \n",
    "        # swap rows if needed\n",
    "        if (max_row != 0):\n",
    "            print ('swapping rows ', j, ' and', j+max_row)\n",
    "            \n",
    "            tmp_A = np.copy(A[j, :])\n",
    "            A[j, :] = np.copy(A[j+max_row, :])\n",
    "            A[j+max_row, :] = np.copy(tmp_A)\n",
    "            \n",
    "            tmp_b = np.copy(b[j])\n",
    "            b[j] = np.copy(b[j+max_row])\n",
    "            b[j+max_row] = np.copy(tmp_b)\n",
    "            \n",
    "            #debug:\n",
    "            #print (A,b)\n",
    "            \n",
    "        pivot = A[j,j]\n",
    "        #print (pivot)\n",
    "        \n",
    "        for i in range(j+1,N):  # Loop 2\n",
    "            mult = -A[i,j]/pivot\n",
    "            \n",
    "            A[i,:] += mult*A[j,:] # Loop 3: update row i>j\n",
    "            b[i] += mult*b[j]  #update RHS vector\n",
    "    \n",
    "            #debug:\n",
    "            #print (A,b)\n",
    "            \n",
    "    # optional: print the upper triangular matrix\n",
    "    #print (A)\n",
    "    #print (b)\n",
    "    \n",
    "    # Backsubstitution (two loops)\n",
    "    x = np.zeros(N)\n",
    "    for i in range(N-1,-1,-1): #from the last row up\n",
    "        x[i] = b[i]/A[i,i]\n",
    "        \n",
    "        for j in range(i+1,N):\n",
    "            x[i] -= A[i,j]*x[j]/A[i,i]\n",
    "    \n",
    "    print('Solution: ', x)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swapping rows  0  and 1\n",
      "swapping rows  1  and 2\n",
      "Solution:  [ 1.61904762 -0.42857143 -1.23809524  1.38095238]\n",
      "[ 0.00000000e+00  0.00000000e+00 -4.44089210e-16 -2.22044605e-16]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0., 1, 4, 1], \\\n",
    "              [3, 4, -1, -1], \\\n",
    "              [1, -4, 1, 5], \\\n",
    "             [2, -2, 1, 3]])\n",
    "\n",
    "b = np.array([-4., 3, 9, 7])\n",
    "\n",
    "x = GaussianEliminationPivotingBack(A,b)\n",
    "# check the result\n",
    "print (np.dot(A,x)-b) # Ax-b=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial pivoting reduces numerical rounding errors as one is less likely to subtract very large or very small numbers. <br>\n",
    "In the code above, we kept track of loops (each performing order $N$ operations) in both Gaussian elimination and backsubstitution.<br> \n",
    "This allows us to estimate the computational cost:\n",
    "- Gaussian elimination has 3 nested loops, one of which is hidden in the code above by a vectorized NumPy operation. We thus expect a computational cost of order $N^3$. More precisely, one can shown that the computational cost of Gaussian elimination is $N^3/3$ for a linear system of size $N\\times N$.\n",
    "- Backsubstitution has only 2 nested loops, so its computational cost is $N^2$ and is not dominant for large systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 LU Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *decomposition methods*, we rewrite the matrix $A$ as a product of two or more matrices, chosen to obtain a set of easy-to-solve problems. We will show below that Gaussian elimination is equivalent to decomposing the original matrix as $A = LU$, namely as the product of a lower-triangular matrix $L$ and an upper triangular matrix $U$. Storing and using the matrices $L$ and $U$ makes it possible to perform efficiently a wide range of linear algebra calculations.<br> \n",
    "\n",
    "In Gaussian elimination, we performed row operations to transform the linear system $ A {\\bf x} = {\\bf b} $\n",
    "to an equivalent system that was easy to solve.<br> \n",
    "The matrix $A$ was transformed to an upper triangular matrix $U$, and we solved the corresponding system by back substitution. \n",
    "\n",
    "The row operations performed on $A$ during Gaussian elimination can equivalently be written as a series of matrix multiplications. Annihilating the entries in column $j$ below pivot $a_{jj}$ can be achieved by multiplying $A$ through a lower-triangular annihilation matrix $M_j$: \n",
    "\n",
    "$$\n",
    "M_j = \\begin{bmatrix} \n",
    "    1      & \\ldots & 0 &  0 & \\ldots & 0\\\\\n",
    "    \\vdots & \\ddots & \\vdots &  \\vdots & \\ddots & \\vdots  \\\\\n",
    "    0 &  \\ldots &  1  & 0 & \\ldots & 0 \\\\\n",
    "    0 & \\ldots & m_{j+1} & 1 & \\ldots & 0 \\\\\n",
    "    \\vdots & \\ddots & \\vdots &  \\vdots & \\ddots & \\vdots  \\\\\n",
    "    0 & \\ldots & m_N & 0 & \\ldots & 1\n",
    "    \\end{bmatrix} $$\n",
    "\n",
    "\n",
    "with elements $m_i$ equal to the multipliers defined above, $m_i = - a_{ij}\\,/\\,a_{jj}$, for $i = j+1,\\ldots,N$.<br> \n",
    "    \n",
    "The row operations performed in Gaussian elimination without pivoting are equivalent to applying $N-1$ lower triangular matrices $M_j$ to matrix $A$, which reduces it to an upper triangular matrix $U$: \n",
    "\n",
    "$$ (M_{N-1} \\ldots M_1)\\, A = U $$\n",
    "\n",
    "The inverse of each lower triangular matrix $M_j$ is still lower triangular, and we write it as $L_j = M_j^{-1}$. \n",
    "The product of all matrices $L_j$ is the lower triangular matrix $L = L_1 \\,L_2 \\ldots L_N$ with diagonal elements equal to 1 and non-zero elements below the diagonal. Therefore, we can write:\n",
    "\n",
    "$$ A = (M_{N-1} \\ldots M_1)^{-1}\\,U = (L_1 \\,L_2 \\ldots L_N)\\,U = LU$$\n",
    "\n",
    "In practice, one can decompose $A$ in its $LU$ form without multiplying all the matrices $M_j$, but simply by keeping track of the lower-diagonal entries (multipliers) used to reduce $A$ to upper triangular form during Gaussian elimination. Typically, the matrices $L$ and $U$ are stored together by overwriting $A$ (the diagonal elements of $L$ are equal to 1 and need not be stored). \n",
    "\n",
    "Once we obtain the $A=LU$ decomposition, the linear system can be solved for an arbitrary number of righthand side (RHS) vectors $\\mathbf{b}$ as the equivalent two systems:\n",
    "\n",
    "\\begin{equation}\n",
    "  A {\\bf x} = {\\bf b} \\qquad \\Leftrightarrow \\qquad \\left\\{ \\begin{aligned} L {\\bf y} & = {\\bf b}, \\\\ U {\\bf x} & = {\\bf y}. \\end{aligned} \\right. \n",
    "\\end{equation}\n",
    "\n",
    "As both $L$ and $U$ are triangular, the two systems they define can easily be solved using forwards and backward substitution respectively.<br> By storing the matrices $L$ and $U$, we can solve the system $A \\mathbf{x} = \\mathbf{b}$ for any number of RHS vectors without performing Gaussian elimination each time. \n",
    "\n",
    "The great advantage is that while Gaussian elimination and $LU$ decomposition each cost $\\mathcal{O}(N^3)$,  performing back or forward substitution costs $\\mathcal{O}(N^2)$.<br> \n",
    "The [`LAPACK` routine `dgesv`](http://www.netlib.org/lapack/explore-html/d7/d3b/group__double_g_esolve_ga5ee879032a8365897c3ba91e3dc8d512.html) performs $LU$ decomposition and returns the matrix $A$ in its $LU$ decomposed form and the solutions to $A \\mathbf{x} = \\mathbf{b}$ for any number of RHS vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, one may wonder when $LU$ decomposition can be performed. One can show that if a matrix is nonsingular, $LU$ decomposition exists provided one performs partial pivoting so that all elements $u_{jj} \\ne 0$. \n",
    "In practice, a condition that is much easier to check is that a matrix is *strictly diagonally dominant*, namely that the diagonal is greater in magnitude than the sum of all other elements in the row:\n",
    "$$| a_{i i} | > \\sum_{\\substack{j \\ne i}}^n | a_{i j} |,\n",
    "        \\quad (1 \\le i \\le n).$$\n",
    "\n",
    "One can show that every strictly diagonally dominant matrix is nonsingular and has an $LU$ decomposition. \n",
    "Diagonal dominance is a very simple condition to check. Most matrices involved in numerical methods for solving PDEs are diagonally dominant. \n",
    "\n",
    "The $LU$ decomposition is in general not unique as $L$ and $U$ together have $N^2+N$ free coefficients while $A$ only has $N^2$. We can freely choose $N$ coefficients. The Doolittle approach discussed below removes this ambiguity by setting the diagonal elements of $L$ to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below performs $LU$ decomposition using the [Doolittle algorithm](https://www.geeksforgeeks.org/doolittle-algorithm-lu-decomposition/). By multiplying the elements of $L$ and $U$ and equating them to $A$, one can obtain $L$ and $U$ by looping over rows and columns using three nested loops, with cost of order $N^3$. This example with a $3\\times3$ matrix illustrates the approach.\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{ccc}\n",
    "    1  & 0 & 0 \\\\\n",
    "    l_{21} & 1 & 0 \\\\\n",
    "    l_{31} & l_{32} & 1 \n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{ccc}\n",
    "    u_{11}  & u_{12} & u_{13} \\\\\n",
    "    0  & u_{22} & u_{23} \\\\\n",
    "    0  & 0 & u_{33}\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{ccc}\n",
    "    a_{11}  & a_{12} & a_{13} \\\\\n",
    "    a_{21}  & a_{22} & a_{23} \\\\\n",
    "    a_{31}  & a_{32} & a_{33}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "The matrix elements of $L$ and $U$ working from the first row / column of $A$ to the last:<br>\n",
    "\n",
    "$\\text{1st row:}\\,\\,\\,\\,\\, u_{11} = a_{11}\\,, \\,\\,\\,\\,\\, u_{12} = a_{12}\\,, \\,\\,\\,\\,\\, u_{13} = a_{13} $<br>\n",
    "\n",
    "$\\text{1st column:}\\,\\,\\,\\,\\, l_{21}u_{11}  = a_{21}\\,, \\,\\,\\,\\,\\, l_{31}u_{11}  = a_{31} $<br>\n",
    "\n",
    "$\\text{2nd row:}\\,\\,\\,\\,\\, l_{21}u_{12}  + u_{22} = a_{22}\\,, \\,\\,\\,\\,\\, l_{21}u_{13}  + u_{23} = a_{23} $<br>\n",
    "\n",
    "$\\text{2nd column:}\\,\\,\\,\\,\\, l_{31}u_{13}  + l_{32} u_{23} = a_{32} $<br>\n",
    "\n",
    "$\\text{3rd row:}\\,\\,\\,\\,\\,  l_{31}u_{13} + l_{32} u_{23}  + u_{33} = a_{33} $<br>\n",
    "\n",
    "\n",
    "More general formulas can be found at the Doolittle algorithm link given above.<br>\n",
    "This algorithm fails if the pivot $u_{ii}=0$. As with Gaussian elimination, pivoting is necessary for general matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LU decomposition using the Doolittle factorization\n",
    "def LU_decomposition(A):\n",
    "    \n",
    "    L = np.zeros_like(A)\n",
    "    U = np.zeros_like(A)\n",
    "    N = np.size(A, 0)\n",
    "    \n",
    "    for k in range(N):\n",
    "        #diagonal elements of L and U\n",
    "        L[k, k] = 1\n",
    "        U[k, k] = (A[k, k] - np.dot(L[k, :k], U[:k, k])) / L[k, k]\n",
    "        \n",
    "        # elements above (U) or below (L) the diagonal\n",
    "        for j in range(k+1, N):\n",
    "            U[k, j] = (A[k, j] - np.dot(L[k, :k], U[:k, j])) / L[k, k]\n",
    "        for i in range(k+1, N):\n",
    "            L[i, k] = (A[i, k] - np.dot(L[i, :k], U[:k, k])) / U[k, k]\n",
    "    \n",
    "    return L, U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We validate this code for the example matrix used above. While this simple code does not include pivoting (row permutation),<br> \n",
    "accurate $LU$ decomposition code should always implement pivoting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: \n",
      " [[ 1.          0.          0.          0.        ]\n",
      " [ 1.5         1.          0.          0.        ]\n",
      " [ 0.5        -1.8         1.          0.        ]\n",
      " [ 1.         -1.2         0.83823529  1.        ]] \n",
      "\n",
      "U: \n",
      " [[  2.    1.    4.    1. ]\n",
      " [  0.    2.5  -7.   -2.5]\n",
      " [  0.    0.  -13.6   0. ]\n",
      " [  0.    0.    0.   -1. ]] \n",
      "\n",
      "check:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2., 1, 4, 1], \\\n",
    "              [3, 4, -1, -1], \\\n",
    "              [1, -4, 1, 5], \\\n",
    "             [2, -2, 1, 3]])\n",
    "\n",
    "L, U = LU_decomposition(A)\n",
    "print ('L: \\n',L,'\\n')\n",
    "print ('U: \\n',U,'\\n')\n",
    "\n",
    "print ('check:')\n",
    "print (np.dot(L,U) - A,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pivoting in LU decomposition.** When pivoting is performed in $LU$ decomposition, swapping two rows can be achieved by multiplying through a permutation matrix $P$. For example, the matrix that interchanges the first two rows of a $3 \\times 3$ matrix can be written as:\n",
    "\n",
    "$$ P = \\begin{bmatrix} \n",
    "    0 & 1 & 0 \\\\\n",
    "    1 & 0 & 0 \\\\\n",
    "    0 & 0 & 1\n",
    "    \\end{bmatrix}$$\n",
    "    \n",
    "Generalizing the treatment discussed above, each step in the LU decomposition can be seen as a multiplication by a lower-diagonal matrix $M$ multiplied, as needed, by a permutation matrix $P$. Therefore, we can write\n",
    "\n",
    "$$ (M_{N-1}\\,P_{N-1} \\ldots M_1\\,P_1)\\, A = U, $$\n",
    "\n",
    "where $P_k$ is the identity matrix if no permutation is performed at step $k$. Since the product $P = P_{N-1}P_{N-2}\\ldots P_1$ is a permutation matrix performing all necessary row reorderings on $A$, we can rewrite the resulting decomposition as:\n",
    "\n",
    "$$ P A = L U $$\n",
    "\n",
    "In the presence of partial pivoting, the matrix $P$ (or equivalently, a list of row exchanges) is also returned together with the matrices $L$ and $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Determinant and matrix inversion\n",
    "\n",
    "The determinant and the inverse of a matrix can also be computed from its $LU$ decomposition: \n",
    "\n",
    "The determinant of a triangular matrix is simply the product of its diagonal entries. We can obtain $\\det(A)$ from the product of the diagonal entries in $U$: \n",
    "\n",
    "$$\\det(A) = \\det(LU) = \\det(L) \\det(U) = \\det(U) = u_{11} \\times u_{22}\\, \\ldots \\times u_{NN},$$ \n",
    "\n",
    "where we used the fact that $\\det(L)=1$ because its diagonal elements are equal to 1. Using $LU$ decomposition, finding the determinant is an ${\\cal O}(N^3)$ calculation while expansion in minors is an ${\\cal O}(N!)$\n",
    "task.<br>\n",
    "\n",
    "We set up the problem of finding the inverse as finding the matrix $X$ that satisfies $ A X = I $. Solving for the column $j$ of the unknown inverse $X$, defined here as $\\mathbf{x}_j$, is equivalent to solving the linear system $A\\mathbf{x}_j = \\mathbf{I}_j$, where $\\mathbf{I}_j$ is the $j^{\\rm th}$ column vector of the identity matrix, with components all equal to zero except component $j$, which is equal to 1. \n",
    "In principle, one could solve $n$ linear systems to determine the inverse using Gaussian elimination, but that would be very costly. Rather, once we have computed the LU decomposition of matrix $A$, computing the inverse is straightforward. In the lab assignment, you will develop a routine to compute the inverse of a matrix $A$ using LU decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Special matrices\n",
    "So far we have assumed that the matrix $A$ of the linear system is a general matrix and is *dense*, meaning that nearly all of the matrix entries are non-zero.<br> \n",
    "If the matrix has some special properties, then workload and storage can often be saved in solving the linear system. Examples of properties that one can leverage to design an algorithm with lower computational cost and storage include:\n",
    "\n",
    "- Symmetric: $A=A^{T}$, and thus $a_{ij} = a_{ji}$ for all $i,j$. \n",
    "- Hermitian: $A=A^{\\dagger}$, where $A^\\dagger$ is the Hermitian conjugate. This implies $a_{ij}=(a_{ji})^*$ for all $i,j$.\n",
    "- Positive definite: $\\mathbf{x^T}A\\,\\mathbf{x} > 0$ for all $\\mathbf{x} \\ne 0$; equivalently, all the eigenvalues are positive.\n",
    "- Banded: $a_{ij}=0$ for all $|i-j|<\\beta$, where $\\beta$ is the bandwidth of the matrix. An important special case is tridiagonal matrices, for which $\\beta = 1$.\n",
    "- Sparse: most entries of the matrix $A$ are zero.\n",
    "\n",
    "Techniques handling symmetric and banded systems are relatively straightforward variations of Gaussian elimination and $LU$ decomposition for dense systems. An example is Cholesky factorization, the equivalent of $LU$ decomposition for a symmetric (or Hermitian) matrix that is also positive definite.<br>\n",
    "This method is analyzed in the example below and in the Lab assignment. \n",
    "\n",
    "Sparse linear systems, on the other hand, require more sophisticated algorithms and data structures that avoid storing or operating on the zeros in the matrix. Sparse systems are often best solved using *iterative* methods, which are discussed in the next lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Cholesky factorization\n",
    "\n",
    "If the matrix we want to decompose is symmetric and positive definite, the $LU$ factorization can be arranged so that $U=L^T$, that is, we decompose as $A=LL^T$ using only a lower triangular matrix (and its transpose) with positive entries that are in general different from 1.<br> \n",
    "This approach is known as [Cholesky decomposition](https://www.geeksforgeeks.org/cholesky-decomposition-matrix-decomposition/) and is illustrated below with an example for a $2\\times2$ matrix.\n",
    "\n",
    "  \\begin{equation}\n",
    "    L =\n",
    "    \\begin{pmatrix}\n",
    "      l_{11} & 0 \\\\\n",
    "      l_{21} & l_{22}\n",
    "    \\end{pmatrix}\n",
    "  \\end{equation}\n",
    "  \n",
    "  Since $A = L L^T$, we equate the matrix elements explicitly and obtain:\n",
    "  \n",
    "  \\begin{align}\n",
    "                && A & = L L^T \\\\\n",
    "    \\Rightarrow &&\n",
    "    \\begin{pmatrix}\n",
    "      a_{11} & a_{12} \\\\\n",
    "      a_{21} & a_{22}\n",
    "    \\end{pmatrix}\n",
    "    & =\n",
    "    \\begin{pmatrix}\n",
    "      \\ell_{1 1}^2 & \\ell_{1 1} \\ell_{2 1} \\\\\n",
    "      \\ell_{1 1} \\ell_{2 1} & \\ell_{2 1}^2 +  \\ell_{2 2}^2\n",
    "    \\end{pmatrix}\n",
    "  \\end{align}\n",
    "\n",
    "This implies that we can find the elements of $L$ working from the first row / column to the last:\n",
    "\n",
    "$$l_{11} = \\sqrt{a_{11}},\\,\\,\\,\\,\\, l_{21} = a_{21}\\,/\\,l_{11},\\,\\,\\,\\,\\, l_{22} = \\sqrt{a_{22} - l_{21}^2}$$\n",
    "\n",
    "The great advantage is that Cholesky factorization can be accomplished with only about $N^3/6$ multiplications and additions, and thus about half the computational cost of $LU$ factorization. Only the lower triangular portion of $A$ is needed, and thus the upper triangular portion need not be stored.<br> \n",
    "In the Lab assignment, you will write code to perform the Cholesky factorization of a symmetric matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear algebra libraries and resources\n",
    "\n",
    "- The [LAPACK library](http://www.netlib.org/lapack/) provides a variety of [modules](http://www.netlib.org/lapack/explore-html/modules.html) to solve linear systems and perform a wide range of other linear algebra tasks. LAPACK relies on the lower-level BLAS library that includes matrix-vector and matrix-matrix operations for better utilization of hierarchical CPU memory and optimal data reuse. Generic versions of BLAS and LAPACK are available from Netlib, and many computer vendors (e.g., Intel and Cray) provide custom versions that are optimized for higher performance on their particular system.  \n",
    "- A standard reference for numerical linear algebra is Trefethen, Numerical Linear Algebra, SIAM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
